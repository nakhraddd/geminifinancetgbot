# üèóÔ∏è Architecture Overview

## System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    ACCOUNTING AGENT EVAL SYSTEM                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   User Input     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Accounting      ‚îÇ
‚îÇ                  ‚îÇ         ‚îÇ     Agent        ‚îÇ
‚îÇ - –ó–∞—Ä–ø–ª–∞—Ç–∞?      ‚îÇ         ‚îÇ                  ‚îÇ
‚îÇ - –ù–î–° –≤–æ–ø—Ä–æ—Å?    ‚îÇ         ‚îÇ  (Gemini 2.0     ‚îÇ
‚îÇ - –ë—É—Ö—É—á–µ—Ç?       ‚îÇ         ‚îÇ   Flash)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
                                      ‚îÇ Response
                                      ‚ñº
                             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                             ‚îÇ   Evaluator      ‚îÇ
                             ‚îÇ                  ‚îÇ
                             ‚îÇ  (Gemini 2.0     ‚îÇ
                             ‚îÇ   Thinking)      ‚îÇ
                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
                                      ‚îÇ Evaluation
                                      ‚ñº
                             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                             ‚îÇ   Test Results   ‚îÇ
                             ‚îÇ                  ‚îÇ
                             ‚îÇ - Correctness    ‚îÇ
                             ‚îÇ - Safety         ‚îÇ
                             ‚îÇ - Tone           ‚îÇ
                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Component Breakdown

### 1. Agent Layer (`src/agent/`)

```
AccountingAgent
‚îú‚îÄ‚îÄ config.py          ‚Üí Domain config, JTBD, system prompt
‚îú‚îÄ‚îÄ prompts.py         ‚Üí Task-specific prompts
‚îÇ   ‚îú‚îÄ‚îÄ get_salary_calculation_prompt()
‚îÇ   ‚îú‚îÄ‚îÄ get_vat_consultation_prompt()
‚îÇ   ‚îî‚îÄ‚îÄ get_general_prompt()
‚îî‚îÄ‚îÄ agent.py           ‚Üí Main agent logic
    ‚îú‚îÄ‚îÄ classify_query()
    ‚îú‚îÄ‚îÄ answer()
    ‚îî‚îÄ‚îÄ reset_history()
```

**Responsibilities:**
- Classify user queries (salary/vat/general)
- Select appropriate prompt
- Generate response via Gemini API
- Maintain conversation history

### 2. Evaluation Layer (`src/evals/`)

```
Evaluation System
‚îú‚îÄ‚îÄ evaluator.py                    ‚Üí Gemini-based evaluator
‚îÇ   ‚îú‚îÄ‚îÄ evaluate_salary_correctness()
‚îÇ   ‚îú‚îÄ‚îÄ evaluate_vat_correctness()
‚îÇ   ‚îî‚îÄ‚îÄ evaluate_safety_and_tone()
‚îú‚îÄ‚îÄ test_correctness.py            ‚Üí Pytest correctness tests
‚îÇ   ‚îú‚îÄ‚îÄ TestSalaryCalculationCorrectness
‚îÇ   ‚îî‚îÄ‚îÄ TestVATConsultationCorrectness
‚îî‚îÄ‚îÄ test_safety.py                 ‚Üí Pytest safety tests
    ‚îú‚îÄ‚îÄ TestAgentSafety.test_adversarial_resistance()
    ‚îî‚îÄ‚îÄ TestAgentSafety.test_edge_case_handling()
```

**Responsibilities:**
- Evaluate agent responses for correctness
- Check safety and ethical compliance
- Generate detailed critiques and scores
- Detect hallucinations

### 3. Data Generation Layer (`src/utils/`)

```
Data Pipeline
‚îú‚îÄ‚îÄ data_generator.py
‚îÇ   ‚îú‚îÄ‚îÄ generate_salary_scenarios()     ‚Üí 20 test cases
‚îÇ   ‚îî‚îÄ‚îÄ generate_vat_scenarios()        ‚Üí 20 test cases
‚îî‚îÄ‚îÄ synthetic_conversations.json        ‚Üí Output
    ‚îú‚îÄ‚îÄ salary_calculation[]
    ‚îî‚îÄ‚îÄ vat_consultation[]
```

**Responsibilities:**
- Generate realistic test scenarios
- Calculate expected values
- Validate scenario correctness
- Save to JSON format

### 4. Infrastructure Layer (`src/utils/`)

```
Infrastructure
‚îú‚îÄ‚îÄ gemini_client.py           ‚Üí API wrapper
‚îÇ   ‚îú‚îÄ‚îÄ GeminiClient
‚îÇ   ‚îú‚îÄ‚îÄ get_agent_client()
‚îÇ   ‚îî‚îÄ‚îÄ get_eval_client()
‚îî‚îÄ‚îÄ eval_runner.py             ‚Üí Orchestration
    ‚îú‚îÄ‚îÄ run_all_evals()
    ‚îú‚îÄ‚îÄ generate_report()
    ‚îî‚îÄ‚îÄ collect_all_results()
```

**Responsibilities:**
- Manage Gemini API calls
- Orchestrate evaluation pipeline
- Generate reports
- Collect and aggregate results

## Data Flow

### Evaluation Pipeline Flow

```
1. DATA GENERATION
   ‚îî‚îÄ‚ñ∂ SyntheticDataGenerator
       ‚îî‚îÄ‚ñ∂ Gemini API (generate scenarios)
           ‚îî‚îÄ‚ñ∂ synthetic_conversations.json

2. AGENT EXECUTION
   ‚îî‚îÄ‚ñ∂ AccountingAgent.answer(query)
       ‚îî‚îÄ‚ñ∂ Gemini API (agent model)
           ‚îî‚îÄ‚ñ∂ agent_response

3. EVALUATION
   ‚îî‚îÄ‚ñ∂ GeminiEvaluator.evaluate_*()
       ‚îî‚îÄ‚ñ∂ Gemini API (eval model)
           ‚îî‚îÄ‚ñ∂ eval_result
               ‚îú‚îÄ pass: bool
               ‚îú‚îÄ score: 0-100
               ‚îú‚îÄ errors: []
               ‚îî‚îÄ critique: str

4. REPORTING
   ‚îî‚îÄ‚ñ∂ EvalRunner.generate_report()
       ‚îî‚îÄ‚ñ∂ results/
           ‚îú‚îÄ *_results.json
           ‚îî‚îÄ report.md
```

## Key Design Decisions

### 1. Gemini-Only Architecture

**Why:** Simplicity and consistency
- Single API provider
- Consistent response format
- Lower complexity vs. multi-provider setup

### 2. Two-Model Approach

**Agent Model:** `gemini-2.0-flash-exp`
- Fast, cost-effective
- Good for production use case
- Temperature: 0.7 for natural responses

**Eval Model:** `gemini-2.0-flash-thinking-exp-1219`
- More thorough reasoning
- Better at critique and analysis
- Temperature: 0.0 for deterministic evaluation

### 3. Domain-Specific JTBD

**Jobs To Be Done:**
1. **Salary Calculation** - HR managers, business owners
2. **VAT Consultation** - Accountants, business owners

Benefits:
- Clear success criteria
- Targeted test scenarios
- Measurable improvements

### 4. Multi-Dimensional Evaluation

**Correctness:**
- Mathematical accuracy
- Regulatory compliance
- Completeness of explanation

**Safety:**
- No illegal advice
- Appropriate caution
- Professional boundaries

**Tone:**
- Politeness
- Clarity
- User-friendliness

## Configuration

### Environment Variables

```bash
# Agent Config
AGENT_MODEL=gemini-2.0-flash-exp
AGENT_TEMPERATURE=0.7              # More creative
AGENT_MAX_TOKENS=2048

# Eval Config
EVAL_MODEL=gemini-2.0-flash-thinking-exp-1219
EVAL_TEMPERATURE=0.0               # Deterministic
EVAL_MAX_TOKENS=4096               # Longer analysis
```

### Prompts Structure

```
System Prompt (config.py)
‚îú‚îÄ‚îÄ Role definition
‚îú‚îÄ‚îÄ Responsibilities
‚îú‚îÄ‚îÄ Rules (do's and don'ts)
‚îú‚îÄ‚îÄ –ê–∫—Ç—É–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (2025)
‚îî‚îÄ‚îÄ Format guidelines

Task Prompt (prompts.py)
‚îú‚îÄ‚îÄ Specific task instructions
‚îú‚îÄ‚îÄ Step-by-step calculation guide
‚îú‚îÄ‚îÄ Format requirements
‚îî‚îÄ‚îÄ Edge case handling
```

## Extensibility Points

### Adding New JTBD

1. Define in `src/agent/config.py`:
```python
JTBD_3_INVOICE_VERIFICATION = {
    "id": "jtbd_003",
    "name": "–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—á–µ—Ç–∞-—Ñ–∞–∫—Ç—É—Ä—ã",
    ...
}
```

2. Add prompt in `src/agent/prompts.py`:
```python
def get_invoice_verification_prompt(user_query: str) -> str:
    ...
```

3. Update classifier in `src/agent/agent.py`:
```python
def classify_query(self, user_query: str) -> str:
    ...
    elif '—Å—á–µ—Ç-—Ñ–∞–∫—Ç—É—Ä' in query_lower:
        return 'invoice'
```

4. Add evaluator method in `src/evals/evaluator.py`:
```python
def evaluate_invoice_verification(self, ...):
    ...
```

5. Create test file `src/evals/test_invoice.py`

### Adding New Metrics

1. Add to evaluator:
```python
def evaluate_response_time(self, ...):
    ...
```

2. Update test files to call new metric

3. Update reporting in `eval_runner.py`

## Testing Strategy

### Unit Tests (future)
- Individual component testing
- Mock Gemini API responses
- Edge case validation

### Integration Tests (current)
- Full agent + evaluator pipeline
- Real API calls
- Synthetic data scenarios

### Safety Tests
- Adversarial queries
- Edge cases
- Ethical boundary testing

## Performance Considerations

### API Rate Limits
- Batch processing for large eval runs
- Exponential backoff on rate limits
- Caching where appropriate

### Cost Optimization
- Use Flash model for agent (cheaper)
- Use Thinking model only for eval (when needed)
- Limit token counts appropriately

### Evaluation Time
- ~40 test scenarios
- ~2-3 API calls per scenario
- Estimated time: 5-10 minutes

## Security & Privacy

### API Key Management
- Store in `.env` (never commit)
- Use environment variables
- Rotate keys regularly

### Data Privacy
- No real user data in synthetic scenarios
- Mock company names and amounts
- No PII in test cases

### Safety Guardrails
- Refuse illegal advice
- No legal consultation overreach
- Clear professional boundaries

---

**Last Updated:** 2025-12-06
**Version:** 1.0.0
